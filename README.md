# ğŸš€ PromptCache

### **Reduce your LLM costs. Accelerate your application.**

**A smart semantic cache for high-scale GenAI workloads.**

![Go](https://img.shields.io/badge/Go-1.25+-00ADD8?style=flat\&logo=go)
![License](https://img.shields.io/badge/license-MIT-green)

![PromptCache Demo](assets/demo.png)

> [!WARNING]
> **v0.1.0 is currently in Alpha.** It is not yet production-ready.
> Significant improvements in stability, performance, and configuration are coming in v0.2.0.
---

## ğŸ’° The Problem

In production, **a large percentage of LLM requests are repetitive**:

* **RAG applications**: Variations of the same employee questions
* **AI Agents**: Repeated reasoning steps or tool calls
* **Support Bots**: Thousands of similar customer queries

Every redundant request means **extra token cost** and **extra latency**.

Why pay your LLM provider multiple times for the *same answer*?

---

## ğŸ’¡ The Solution: PromptCache

PromptCache is a lightweight middleware that sits between your application and your LLM provider.
It uses **semantic understanding** to detect when a new prompt has *the same intent* as a previous one â€” and returns the cached result instantly.

---

## ğŸ“Š Key Benefits

| Metric                      | Without Cache | With PromptCache | Benefit      |
| --------------------------- | ------------- | ---------------- | ------------ |
| **Cost per 1,000 Requests** | â‰ˆ $30         | **â‰ˆ $6**         | Lower cost   |
| **Avg Latency**         | ~1.5s         | **~300ms**       | Faster UX    |
| **Throughput**              | API-limited   | **Unlimited**    | Better scale |

Numbers vary per model, but the pattern holds across real workloads:
**semantic caching dramatically reduces cost and latency**.

\* Results may vary depending on model, usage patterns, and configuration.

---

## ğŸ§  Smart Semantic Matching (Safer by Design)

Naive semantic caches can be risky â€” they may return incorrect answers when prompts look similar but differ in intent.

PromptCache uses a **two-stage verification strategy** to ensure accuracy:

1. **High similarity â†’ direct cache hit**
2. **Low similarity â†’ skip cache directly**
3. **Gray zone â†’ intent check using a small, cheap verification model**

This ensures cached responses are **semantically correct**, not just â€œclose enoughâ€.

---

## ğŸš€ Quick Start

PromptCache works as a **drop-in replacement** for the OpenAI API.

### 1. Run with Docker (Recommended)

```bash
# Clone the repo
git clone https://github.com/messkan/prompt-cache.git
cd prompt-cache

# Run with Docker Compose
export OPENAI_API_KEY=your_key_here
docker-compose up -d
```

### 2. Run from Source

Simply change the `base_url` in your SDK:

```python
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:8080/v1",  # Point to PromptCache
    api_key="sk-..."
)

# First request â†’ goes to the LLM provider
client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "Explain quantum physics"}]
)

# Semantically similar request â†’ served from PromptCache
client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "How does quantum physics work?"}]
)
```

No code changes. Just point your client to PromptCache.

---

## ğŸ— Architecture Overview

Built for speed, safety, and reliability:

* **Pure Go implementation** (high concurrency, minimal overhead)
* **BadgerDB** for fast embedded persistent storage
* **In-memory caching** for ultra-fast responses
* **OpenAI-compatible API** for seamless integration
* **Docker Setup**
---

## ğŸ›£ï¸ Roadmap

### âœ”ï¸ v0.1.0 (Released)

* In-memory & BadgerDB storage
* Smart semantic verification (dual-threshold + intent check)
* OpenAI API compatibility

### ğŸš§ v0.2.0 (Planned)

* **Core Improvements**: Bug fixes and performance optimizations.
* **Public API**: Improve cache create/delete operations.
* **Enhanced Configuration**:
    * Configurable "gray zone" fallback model (enable/disable env var).
    * User-definable similarity thresholds with sensible defaults.

### ğŸš§ v0.3.0 (Planned)
* Built-in support for Claude & Mistral APIs

### ğŸš€ v1.0.0

* Clustered mode (Raft or gossip-based replication)
* Custom embedding backends (Ollama, local models)
* Rate-limiting & request shaping
* Web dashboard (hit rate, latency, cost metrics)

### â¤ï¸ Support the Project

We are working hard to reach **v1.0.0**! If you find this project useful, please give it a â­ï¸ on GitHub and consider contributing. Your support helps us ship v0.2.0 and v1.0.0 faster!

---

## ğŸ“„ License

MIT License.
